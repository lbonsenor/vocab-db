{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE INSTALLATION\n",
    "\n",
    "%pip install jupyter stanza supabase dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c06495ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'db' from '/home/boddah/Documents/Stanza/db.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOCAL IMPORTS\n",
    "\n",
    "import importlib\n",
    "import romanizer\n",
    "import db\n",
    "\n",
    "importlib.reload(romanizer)     # Reload romanizer from korean\n",
    "importlib.reload(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee28a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATABASE\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DOWNLOAD\n",
    "\n",
    "import stanza\n",
    "\n",
    "language = 'ko'\n",
    "stanza.download(language)       # Download model\n",
    "nlp = stanza.Pipeline(language) # Initialize neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b97411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481b79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"text\": \"무엇을\",\n",
      "    \"lemma\": \"무엇+을\",\n",
      "    \"upos\": \"PRON\",\n",
      "    \"xpos\": \"npd+jco\",\n",
      "    \"head\": 2,\n",
      "    \"deprel\": \"obj\",\n",
      "    \"start_char\": 0,\n",
      "    \"end_char\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"text\": \"먹을\",\n",
      "    \"lemma\": \"먹+ㄹ\",\n",
      "    \"upos\": \"VERB\",\n",
      "    \"xpos\": \"pvg+etm\",\n",
      "    \"head\": 3,\n",
      "    \"deprel\": \"ccomp\",\n",
      "    \"start_char\": 4,\n",
      "    \"end_char\": 6\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"text\": \"거예요\",\n",
      "    \"lemma\": \"거+이+에요\",\n",
      "    \"upos\": \"VERB\",\n",
      "    \"xpos\": \"nbn+jp+ef\",\n",
      "    \"head\": 0,\n",
      "    \"deprel\": \"root\",\n",
      "    \"start_char\": 7,\n",
      "    \"end_char\": 10,\n",
      "    \"misc\": \"SpaceAfter=No\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"text\": \"?\",\n",
      "    \"lemma\": \"?\",\n",
      "    \"upos\": \"PUNCT\",\n",
      "    \"xpos\": \"sf\",\n",
      "    \"head\": 3,\n",
      "    \"deprel\": \"punct\",\n",
      "    \"start_char\": 10,\n",
      "    \"end_char\": 11,\n",
      "    \"misc\": \"SpaceAfter=No\"\n",
      "  }\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "# PHRASE TOKENIZATION\n",
    "phrase = \"무엇을 먹을 거예요?\"\n",
    "\n",
    "doc = nlp(phrase)\n",
    "dict = doc.to_dict()\n",
    "\n",
    "print(doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c56fe056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase:  무엇을 먹을 거예요?\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m 무엇을 | \u001b[94mmueoseul\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Pronoun\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    '무엇': 'npd => Demonstrative Noun/Pronoun' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: what\n",
      "    '을': 'jco => Object Case Particle' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: (object particle)\n",
      "\u001b[95mTranslation: \u001b[0m what (object)\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m 먹을 | \u001b[94mmeokeul\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Verb\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    '먹': 'pvg => General Verb Root' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: to eat\n",
      "    'ㄹ': 'etm => Adnominal Suffix' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: -going to (future/intentional modifier)\n",
      "\u001b[95mTranslation: \u001b[0m going to eat\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m 거예요 | \u001b[94mgeoyeyo\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Verb\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    '거': 'nbn => Nominal Bound Noun' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: thing\n",
      "    '이': 'jp => Subject Particle' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: (subject particle)\n",
      "    '에요': 'ef => Final Ending' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: to be (informal high)\n",
      "\u001b[95mTranslation: \u001b[0m it is the thing\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m ? | \u001b[94m?\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Punctuation\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    '?': 'sf => Final Punctuation Mark' | \u001b[1m\u001b[91mTranslation\u001b[0m\u001b[0m: ?\n",
      "\u001b[95mTranslation: \u001b[0m ?\n",
      "────────────────────────────── \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from color import blue, bold, cyan, green, purple, red\n",
    "\n",
    "\n",
    "for sentences in dict:\n",
    "    print(\"Phrase: \", phrase)\n",
    "    for words in sentences:\n",
    "        print(bold(cyan(\"Text: \")), words['text'], '|', blue(romanizer.hangul(words['text'])))\n",
    "\n",
    "        upos = db.getUpos(supabase, words['upos'])\n",
    "        print(green(\"Part of Speech: \"), upos)\n",
    "\n",
    "        print(red(\"Morphemes: \"))\n",
    "        morphemes = words['lemma'].split('+')\n",
    "\n",
    "        tags = words['xpos'].split('+')\n",
    "        xpos = db.getXpos(supabase, tags)\n",
    "\n",
    "        morphemes_info = db.getMorphemes(supabase, morphemes, tags)\n",
    "\n",
    "        for morpheme, tag, xpos_label, info in zip(morphemes, tags, xpos, morphemes_info):\n",
    "            print(f\"    '{morpheme}': '{tag} => {xpos_label.title()}' | {bold(red('Translation'))}: {info['translation']}\")\n",
    "\n",
    "        translation = db.getTranslation(supabase, words['text'], words['upos'], morphemes_info)\n",
    "        print(purple(\"Translation: \"), translation)\n",
    "        print(\"─\"*30, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
