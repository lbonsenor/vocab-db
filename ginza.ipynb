{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d84ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL IMPORTS\n",
    "\n",
    "import importlib\n",
    "import romanizer\n",
    "import db\n",
    "\n",
    "importlib.reload(romanizer)     # Reload romanizer from korean\n",
    "importlib.reload(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e16749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DOWNLOAD\n",
    "\n",
    "import spacy\n",
    "import ginza\n",
    "\n",
    "language = 'ja'\n",
    "nlp = spacy.load(\"ja_ginza\") # Initialize neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATABASE\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from psycopg2.extensions import connection\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "conn: connection = db.connect(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE CREATION\n",
    "\n",
    "db.create_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debb41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHRASE TOKENIZATION\n",
    "phrase = \"それはいい考えですね\"\n",
    "\n",
    "doc = nlp(phrase)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text} | {romanizer.romanize(token.text)}\")\n",
    "    print(f\"  Lemma: {token.lemma_}\")\n",
    "    print(f\"  POS: {token.pos_}\")\n",
    "    print(f\"  Fine POS: {token.tag_}\")\n",
    "    print(f\"  Head: {token.head.text}\")\n",
    "    print(f\"  DepRel: {token.dep_}\")\n",
    "    print(f\"  SpaceAfter: {token.whitespace_!r}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09900223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from color import blue, bold, cyan, green, purple, red, yellow\n",
    "\n",
    "print(blue(\"Phrase: \"), phrase, '|', red(romanizer.romanize(phrase)))\n",
    "words = romanizer.romanize(phrase).split(' ')\n",
    "\n",
    "for token in doc:\n",
    "    # This is a specific lemma\n",
    "    print(bold(cyan(\"Text: \")), token.text, '|', blue(romanizer.romanize(token.text)))\n",
    "    current_word += romanizer.romanize(token.text)\n",
    "\n",
    "    upos = db.get_upos(conn, token.pos_)\n",
    "    xpos = db.get_xpos(conn, [token.tag_])\n",
    "    print(green(\"Part of Speech: \"), upos, '-', xpos)\n",
    "\n",
    "    print(yellow(\"Kanjis: \"))\n",
    "    for char in token.text:\n",
    "        if (romanizer.is_kanji(char)):\n",
    "            meaning = db.get_etymology(conn, f\"{char}\")\n",
    "            print(f\"    '{char}' => {bold(yellow(meaning))}\")\n",
    "    \n",
    "    print(red(\"Morphemes: \"))\n",
    "    morphemes_info = db.get_morphemes(conn, [token.text], [token.tag_])\n",
    "    for morpheme, tag, xpos_label, info in zip([token.text], [token.tag_], xpos, morphemes_info):\n",
    "        print(f\"    '{morpheme}': '{tag} => {xpos_label.title()}'\")\n",
    "\n",
    "    translation = db.get_translation(conn, token.text, token.pos_, morphemes_info)\n",
    "    print(purple(\"Translation: \"), translation)\n",
    "    print(\"─\"*30, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
