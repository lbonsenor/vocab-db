{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e4d84ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'db' from 'd:\\\\vocab-db\\\\db.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOCAL IMPORTS\n",
    "\n",
    "import importlib\n",
    "import romanizer\n",
    "import db\n",
    "\n",
    "importlib.reload(romanizer)     # Reload romanizer from korean\n",
    "importlib.reload(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e16749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DOWNLOAD\n",
    "\n",
    "import spacy\n",
    "import ginza\n",
    "\n",
    "language = 'ja'\n",
    "nlp = spacy.load(\"ja_ginza\") # Initialize neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b095ce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "# DATABASE\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from psycopg2.extensions import connection\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "conn: connection = db.connect(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE CREATION\n",
    "\n",
    "db.create_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7debb41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: それ | sore\n",
      "  Lemma: それ\n",
      "  POS: PRON\n",
      "  Fine POS: 代名詞\n",
      "  Head: 考え\n",
      "  DepRel: nsubj\n",
      "  SpaceAfter: ''\n",
      "---\n",
      "Text: は | ha\n",
      "  Lemma: は\n",
      "  POS: ADP\n",
      "  Fine POS: 助詞-係助詞\n",
      "  Head: それ\n",
      "  DepRel: case\n",
      "  SpaceAfter: ''\n",
      "---\n",
      "Text: いい | ii\n",
      "  Lemma: いい\n",
      "  POS: ADJ\n",
      "  Fine POS: 形容詞-非自立可能\n",
      "  Head: 考え\n",
      "  DepRel: acl\n",
      "  SpaceAfter: ''\n",
      "---\n",
      "Text: 考え | kangae\n",
      "  Lemma: 考え\n",
      "  POS: NOUN\n",
      "  Fine POS: 名詞-普通名詞-一般\n",
      "  Head: 考え\n",
      "  DepRel: ROOT\n",
      "  SpaceAfter: ''\n",
      "---\n",
      "Text: です | desu\n",
      "  Lemma: です\n",
      "  POS: AUX\n",
      "  Fine POS: 助動詞\n",
      "  Head: 考え\n",
      "  DepRel: cop\n",
      "  SpaceAfter: ''\n",
      "---\n",
      "Text: ね | ne\n",
      "  Lemma: ね\n",
      "  POS: PART\n",
      "  Fine POS: 助詞-終助詞\n",
      "  Head: 考え\n",
      "  DepRel: mark\n",
      "  SpaceAfter: ''\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# PHRASE TOKENIZATION\n",
    "phrase = \"それはいい考えですね\"\n",
    "\n",
    "doc = nlp(phrase)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text} | {romanizer.romanize(token.text)}\")\n",
    "    print(f\"  Lemma: {token.lemma_}\")\n",
    "    print(f\"  POS: {token.pos_}\")\n",
    "    print(f\"  Fine POS: {token.tag_}\")\n",
    "    print(f\"  Head: {token.head.text}\")\n",
    "    print(f\"  DepRel: {token.dep_}\")\n",
    "    print(f\"  SpaceAfter: {token.whitespace_!r}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09900223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPhrase: \u001b[0m それはいい考えですね | \u001b[91msorehaii kangae desune\u001b[0m\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m それ | \u001b[94msore\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Pronoun - ['pronoun']\n",
      "\u001b[93mKanjis: \u001b[0m\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    'それ': '代名詞 => Pronoun'\n",
      "\u001b[95mTranslation: \u001b[0m that\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m は | \u001b[94mha\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Adposition - ['binding particle']\n",
      "\u001b[93mKanjis: \u001b[0m\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    'は': '助詞-係助詞 => Binding Particle'\n",
      "\u001b[95mTranslation: \u001b[0m (topic marker)\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m いい | \u001b[94mii\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Adjective - ['dependent adjective']\n",
      "\u001b[93mKanjis: \u001b[0m\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    'いい': '形容詞-非自立可能 => Dependent Adjective'\n",
      "\u001b[95mTranslation: \u001b[0m good\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m 考え | \u001b[94mkangae\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Noun - ['general common noun']\n",
      "\u001b[93mKanjis: \u001b[0m\n",
      "    '考' => \u001b[1m\u001b[93mconsider, think over\u001b[0m\u001b[0m\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    '考え': '名詞-普通名詞-一般 => General Common Noun'\n",
      "\u001b[95mTranslation: \u001b[0m thought/idea\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m です | \u001b[94mdesu\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Auxiliary - ['auxiliary verb']\n",
      "\u001b[93mKanjis: \u001b[0m\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    'です': '助動詞 => Auxiliary Verb'\n",
      "\u001b[95mTranslation: \u001b[0m to be (polite)\n",
      "────────────────────────────── \n",
      "\n",
      "\u001b[1m\u001b[96mText: \u001b[0m\u001b[0m ね | \u001b[94mne\u001b[0m\n",
      "\u001b[92mPart of Speech: \u001b[0m Particle - ['ending particle']\n",
      "\u001b[93mKanjis: \u001b[0m\n",
      "\u001b[91mMorphemes: \u001b[0m\n",
      "    'ね': '助詞-終助詞 => Ending Particle'\n",
      "\u001b[95mTranslation: \u001b[0m right?\n",
      "────────────────────────────── \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from color import blue, bold, cyan, green, purple, red, yellow\n",
    "\n",
    "print(blue(\"Phrase: \"), phrase, '|', red(romanizer.romanize(phrase)))\n",
    "words = romanizer.romanize(phrase).split(' ')\n",
    "\n",
    "for token in doc:\n",
    "    # This is a specific lemma\n",
    "    print(bold(cyan(\"Text: \")), token.text, '|', blue(romanizer.romanize(token.text)))\n",
    "    current_word += romanizer.romanize(token.text)\n",
    "\n",
    "    upos = db.get_upos(conn, token.pos_)\n",
    "    xpos = db.get_xpos(conn, [token.tag_])\n",
    "    print(green(\"Part of Speech: \"), upos, '-', xpos)\n",
    "\n",
    "    print(yellow(\"Kanjis: \"))\n",
    "    for char in token.text:\n",
    "        if (romanizer.is_kanji(char)):\n",
    "            meaning = db.get_etymology(conn, f\"{char}\")\n",
    "            print(f\"    '{char}' => {bold(yellow(meaning))}\")\n",
    "    \n",
    "    print(red(\"Morphemes: \"))\n",
    "    morphemes_info = db.get_morphemes(conn, [token.text], [token.tag_])\n",
    "    for morpheme, tag, xpos_label, info in zip([token.text], [token.tag_], xpos, morphemes_info):\n",
    "        print(f\"    '{morpheme}': '{tag} => {xpos_label.title()}'\")\n",
    "\n",
    "    # translation = db.get_translation(conn, token.text, token.pos_, morphemes_info)\n",
    "    print(purple(\"Translation: \"), info['translation'])\n",
    "    print(\"─\"*30, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
